"""Training data utilities for ML model development.

This module provides functions for loading and validating the synthetic
training data generated by scripts/generate_training_data.py.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

import pandas as pd
from loguru import logger

# Default paths
DATA_DIR = Path(__file__).parent.parent.parent / "data" / "processed"
TRAINING_DATA_PATH = DATA_DIR / "training_data.parquet"
TEST_DATA_PATH = DATA_DIR / "test_data.parquet"

# Expected columns in training data
EXPECTED_COLUMNS = [
    "number_of_riders",
    "number_of_drivers",
    "location_category",
    "customer_loyalty_status",
    "number_of_past_rides",
    "average_ratings",
    "time_of_booking",
    "vehicle_type",
    "expected_ride_duration",
    "historical_cost_of_ride",
    "supply_demand_ratio",
    "segment",
    "price",
    "demand",
    "profit",
]

# Feature columns (context features for ML model input)
FEATURE_COLUMNS = [
    "number_of_riders",
    "number_of_drivers",
    "location_category",
    "customer_loyalty_status",
    "number_of_past_rides",
    "average_ratings",
    "time_of_booking",
    "vehicle_type",
    "expected_ride_duration",
    "historical_cost_of_ride",
    "supply_demand_ratio",
    "segment",
    "price",
]

# Target column for demand prediction
TARGET_COLUMN = "demand"

# Categorical columns that need encoding for ML
CATEGORICAL_COLUMNS = [
    "location_category",
    "customer_loyalty_status",
    "time_of_booking",
    "vehicle_type",
    "segment",
]


def load_training_data(path: Path | str | None = None) -> pd.DataFrame:
    """Load the training dataset from parquet file.

    Args:
        path: Path to training data parquet. Defaults to
              backend/data/processed/training_data.parquet.

    Returns:
        DataFrame containing the training dataset.

    Raises:
        FileNotFoundError: If the parquet file does not exist.
        ValueError: If required columns are missing.
    """
    data_path = Path(path) if path else TRAINING_DATA_PATH

    if not data_path.exists():
        raise FileNotFoundError(
            f"Training data not found: {data_path}. "
            "Run scripts/generate_training_data.py first."
        )

    logger.info(f"Loading training data from {data_path}")
    df = pd.read_parquet(data_path)

    # Validate columns
    _validate_columns(df, "training")

    logger.info(f"Loaded {len(df)} training samples")
    return df


def load_test_data(path: Path | str | None = None) -> pd.DataFrame:
    """Load the test dataset from parquet file.

    Args:
        path: Path to test data parquet. Defaults to
              backend/data/processed/test_data.parquet.

    Returns:
        DataFrame containing the test dataset.

    Raises:
        FileNotFoundError: If the parquet file does not exist.
        ValueError: If required columns are missing.
    """
    data_path = Path(path) if path else TEST_DATA_PATH

    if not data_path.exists():
        raise FileNotFoundError(
            f"Test data not found: {data_path}. "
            "Run scripts/generate_training_data.py first."
        )

    logger.info(f"Loading test data from {data_path}")
    df = pd.read_parquet(data_path)

    # Validate columns
    _validate_columns(df, "test")

    logger.info(f"Loaded {len(df)} test samples")
    return df


def _validate_columns(df: pd.DataFrame, dataset_name: str) -> None:
    """Validate that DataFrame has all required columns.

    Args:
        df: DataFrame to validate.
        dataset_name: Name for error messages ('training' or 'test').

    Raises:
        ValueError: If required columns are missing.
    """
    missing_cols = set(EXPECTED_COLUMNS) - set(df.columns)
    if missing_cols:
        raise ValueError(
            f"Missing required columns in {dataset_name} data: {missing_cols}"
        )


def validate_training_data(df: pd.DataFrame) -> dict[str, Any]:
    """Validate training data quality and return statistics.

    Performs comprehensive validation:
    - Checks for missing values
    - Validates value ranges
    - Checks for duplicates
    - Reports segment distribution

    Args:
        df: Training DataFrame to validate.

    Returns:
        Dictionary with validation results and statistics.
    """
    results: dict[str, Any] = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "statistics": {},
    }

    # Check missing values
    missing = df.isnull().sum()
    if missing.sum() > 0:
        cols_with_missing = missing[missing > 0].to_dict()
        results["errors"].append(f"Missing values found: {cols_with_missing}")
        results["valid"] = False

    # Validate demand range [0, 1]
    if df["demand"].min() < 0 or df["demand"].max() > 1:
        results["errors"].append(
            f"Demand values out of range [0, 1]: "
            f"min={df['demand'].min()}, max={df['demand'].max()}"
        )
        results["valid"] = False

    # Validate price is positive
    if df["price"].min() <= 0:
        results["errors"].append(f"Non-positive prices found: min={df['price'].min()}")
        results["valid"] = False

    # Check for duplicate rows
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        results["warnings"].append(f"Found {duplicates} duplicate rows")

    # Collect statistics
    results["statistics"] = {
        "total_samples": len(df),
        "unique_segments": df["segment"].nunique(),
        "segment_distribution": df["segment"].value_counts().to_dict(),
        "price_stats": {
            "min": float(df["price"].min()),
            "max": float(df["price"].max()),
            "mean": float(df["price"].mean()),
            "std": float(df["price"].std()),
        },
        "demand_stats": {
            "min": float(df["demand"].min()),
            "max": float(df["demand"].max()),
            "mean": float(df["demand"].mean()),
            "std": float(df["demand"].std()),
        },
        "profit_stats": {
            "min": float(df["profit"].min()),
            "max": float(df["profit"].max()),
            "mean": float(df["profit"].mean()),
            "std": float(df["profit"].std()),
        },
    }

    if results["valid"]:
        logger.info("Training data validation passed")
    else:
        logger.warning(f"Training data validation failed: {results['errors']}")

    return results


def get_features_and_target(
    df: pd.DataFrame,
) -> tuple[pd.DataFrame, pd.Series]:
    """Split DataFrame into features and target.

    Args:
        df: Training or test DataFrame.

    Returns:
        Tuple of (features DataFrame, target Series).
    """
    X = df[FEATURE_COLUMNS].copy()
    y = df[TARGET_COLUMN].copy()
    return X, y


def get_training_statistics(
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
) -> dict[str, Any]:
    """Get comprehensive statistics about the training/test split.

    Args:
        train_df: Training DataFrame.
        test_df: Test DataFrame.

    Returns:
        Dictionary with split statistics and segment distributions.
    """
    total = len(train_df) + len(test_df)

    return {
        "total_samples": total,
        "train_samples": len(train_df),
        "test_samples": len(test_df),
        "train_ratio": len(train_df) / total,
        "test_ratio": len(test_df) / total,
        "train_segments": train_df["segment"].value_counts().to_dict(),
        "test_segments": test_df["segment"].value_counts().to_dict(),
        "unique_segments": train_df["segment"].nunique(),
    }

