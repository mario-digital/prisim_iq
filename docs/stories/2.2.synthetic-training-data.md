# Story 2.2: Synthetic Training Data Generation

## Status: Done

## Assigned To: Mario

## Story

**As a** data scientist,
**I want** synthetic demand labels generated for the entire dataset,
**so that** I have labeled training data.

## Acceptance Criteria

1. For each row in dataset, generate demand at multiple price points (e.g., 10 price levels)
2. Price points span reasonable range (e.g., $5 to $100 in $10 increments)
3. Training dataset includes: context features, price, simulated demand, calculated profit
4. Profit calculated as `(price - cost) * demand`
5. Train/test split (80/20) with stratification by segment
6. Data saved to `/backend/data/processed/training_data.parquet`
7. Generation reproducible with random seed (default: 42)

## Tasks / Subtasks

- [x] Task 1: Create data generation script (AC: 1, 7)
  - [x] Create `backend/scripts/generate_training_data.py`
  - [x] Set random seed for reproducibility (default: 42)
  - [x] Load original dataset using preprocessor

- [x] Task 2: Generate price points (AC: 2)
  - [x] Define price range: $5 to $100
  - [x] Create 10 price levels at $10 increments
  - [x] Or use percentage-based: 50%, 75%, 100%, 125%, 150% of historical cost

- [x] Task 3: Generate demand labels (AC: 1, 3)
  - [x] For each row, iterate through price points
  - [x] Call demand_simulator.simulate_demand() for each price
  - [x] Store context features + price + demand

- [x] Task 4: Calculate profit (AC: 4)
  - [x] Add profit column: `(price - historical_cost) * demand`
  - [x] Handle edge cases (negative profit possible)

- [x] Task 5: Split and save data (AC: 5, 6)
  - [x] Create `backend/data/processed/` directory
  - [x] Stratified split by segment (80/20)
  - [x] Save to `training_data.parquet`
  - [x] Save test set to `test_data.parquet`
  - [x] Log dataset statistics

- [x] Task 6: Create utility functions
  - [x] `load_training_data()` function
  - [x] `load_test_data()` function
  - [x] Data validation checks

- [x] Task 7: Write tests
  - [x] Test data generation produces correct shape
  - [x] Test profit calculation
  - [x] Test reproducibility with seed

## Dev Notes

### Data Generation Process
```python
# For each original row, generate multiple price-demand pairs
original_rows = 1000  # example
price_points = 10
generated_rows = 1000 * 10 = 10,000

# Output columns
columns = [
    # Original context features
    "number_of_riders", "number_of_drivers", "location_category",
    "customer_loyalty_status", "number_of_past_rides", "average_ratings",
    "time_of_booking", "vehicle_type", "expected_ride_duration",
    "historical_cost_of_ride", "segment",
    # Generated columns
    "price",           # The tested price point
    "demand",          # Simulated demand at this price
    "profit"           # (price - cost) * demand
]
```

### Price Point Strategy
```python
# Option 1: Fixed range
price_points = [5, 15, 25, 35, 45, 55, 65, 75, 85, 95]

# Option 2: Relative to historical cost (preferred)
def get_price_points(historical_cost: float) -> list[float]:
    multipliers = [0.5, 0.75, 0.9, 1.0, 1.1, 1.25, 1.5, 1.75, 2.0, 2.5]
    return [historical_cost * m for m in multipliers]
```

### File Locations
- Script: `backend/scripts/generate_training_data.py`
- Output: `backend/data/processed/training_data.parquet`
- Test set: `backend/data/processed/test_data.parquet`

### Running the Script
```bash
cd backend
python scripts/generate_training_data.py --seed 42
```

### Testing

- Test file: `backend/tests/unit/test_scripts/test_generate_training_data.py`
- Test shape: output rows = input rows × price points
- Test columns exist
- Test reproducibility: same seed = same output

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |
| 2025-12-02 | 1.1 | Implementation complete - training data generation script, utility functions, tests | Dev Agent |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5

### Debug Log References
- `pytest tests/unit/test_scripts/test_generate_training_data.py -v` → 18 passed
- `pytest tests/ -v` → 114 passed (full regression)
- `python scripts/generate_training_data.py --seed 42` → 10,000 samples generated

### Completion Notes List
1. Created `backend/scripts/generate_training_data.py` - Main script for generating synthetic training data
2. Uses percentage-based price multipliers (0.5× to 2.5× historical cost) for 10 price points per row
3. Added `LOYALTY_STATUS_MAP` to handle "Regular" loyalty status in dataset (maps to "Bronze")
4. Created `backend/src/ml/training_data.py` with utility functions:
   - `load_training_data()` - Load training parquet
   - `load_test_data()` - Load test parquet
   - `validate_training_data()` - Data quality checks
   - `get_features_and_target()` - Split X/y for ML
5. Generated 10,000 samples (1000 rows × 10 price points):
   - Training set: 8,000 samples (80%)
   - Test set: 2,000 samples (20%)
   - Stratified by segment (5 unique segments)
6. Added pyarrow dependency for parquet support

### File List
| Action | File |
|--------|------|
| Created | `backend/scripts/__init__.py` |
| Created | `backend/scripts/generate_training_data.py` |
| Created | `backend/src/ml/training_data.py` |
| Created | `backend/tests/unit/test_scripts/__init__.py` |
| Created | `backend/tests/unit/test_scripts/test_generate_training_data.py` |
| Created | `backend/data/processed/training_data.parquet` |
| Created | `backend/data/processed/test_data.parquet` |
| Modified | `backend/requirements.lock` (added pyarrow)

## QA Results

### Review Date: 2025-12-02

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent** - Clean, well-documented implementation with comprehensive test coverage.

**Strengths:**
- Excellent docstrings with Google-style documentation throughout
- Clean separation of concerns: script (`generate_training_data.py`) vs utility module (`training_data.py`)
- Comprehensive type hints on all functions
- Proper error handling with meaningful error messages (FileNotFoundError, ValueError)
- Good use of constants (PRICE_MULTIPLIERS, EXPECTED_COLUMNS, FEATURE_COLUMNS)
- Effective logging with progress indicators and statistics
- Smart design choice: percentage-based price multipliers (0.5× to 2.5× of historical cost) instead of fixed range - more realistic for varying base prices

**Implementation Highlights:**
- Generated 10,000 samples (1000 rows × 10 price points)
- Perfect 80/20 train/test split (8000/2000) with proper stratification
- 5 unique segments maintained across split
- Price range: $13.00 - $2090.29 (appropriate given historical cost variation)
- Demand range: 0.03 - 1.0 (valid probability range)
- Handled "Regular" loyalty status mapping to "Bronze" appropriately

### Refactoring Performed

- **File**: `backend/scripts/generate_training_data.py`
  - **Change**: Added `LoyaltyStatus` type alias and proper type annotations for `LOYALTY_STATUS_MAP`
  - **Why**: mypy reported type mismatch between `str` and `Literal['Bronze', 'Silver', 'Gold', 'Platinum']`
  - **How**: Added `Literal` import, created type alias, annotated dict and variable

- **File**: `backend/scripts/generate_training_data.py`
  - **Change**: Cast `raw_loyalty` to `str` explicitly
  - **Why**: pandas Series type was incompatible with dict `.get()` method
  - **How**: Added `str()` wrapper around `row["Customer_Loyalty_Status"]`

- **File**: `backend/scripts/generate_training_data.py`
  - **Change**: Replaced `df.iterrows()` loop with `enumerate(df.iterrows(), start=1)`
  - **Why**: `idx` from iterrows is `Hashable`, not `int` - `idx + 1` was type-unsafe
  - **How**: Used enumerate to get explicit integer row counter

### Compliance Check

- Coding Standards: ✓ Follows Python backend standards (loguru logging, type hints, docstrings)
- Project Structure: ✓ Files placed in correct locations per project structure
- Testing Strategy: ✓ 18 unit tests following testing pyramid (unit level appropriate for this story)
- All ACs Met: ✓ All 7 acceptance criteria fully implemented and verified

### Improvements Checklist

- [x] All acceptance criteria implemented
- [x] Comprehensive test coverage (18 tests)
- [x] Proper error handling and validation
- [x] Data quality validation utilities provided
- [x] Reproducibility verified with seed parameter
- [x] Type annotations fixed for mypy compliance (performed during review)
- [ ] Consider adding script as proper package entry point (minor - current sys.path approach works)

### Security Review

**Status: N/A** - This is a pure data processing module with no external inputs, authentication, or user data handling. No security concerns applicable.

### Performance Considerations

**Status: Acceptable** - O(n × m) complexity where n = dataset rows, m = price points.
- Current: 1000 rows × 10 price points = 10,000 samples
- Processing completes in seconds
- Parquet format chosen for efficient storage and fast loading

### Files Modified During Review

| Action | File | Change |
|--------|------|--------|
| Modified | `backend/scripts/generate_training_data.py` | Added type annotations for mypy compliance |

**Note to Dev**: Please update the File List section above to include this modification.

### Gate Status

Gate: **PASS** → `docs/qa/gates/2.2-synthetic-training-data.yml`

### Recommended Status

✓ **Ready for Done** - All acceptance criteria met with excellent test coverage and code quality.

