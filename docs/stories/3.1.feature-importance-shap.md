# Story 3.1: Feature Importance Calculation

## Status: Done

## Assigned To: Mario

## Story

**As a** pricing analyst,
**I want** to see which factors most influenced the recommendation,
**so that** I can understand and explain decisions to stakeholders.

## Acceptance Criteria

1. Feature importance extracted from Random Forest/XGBoost `feature_importances_` attribute
2. Linear Regression coefficients normalized to percentage scale
3. Decision Tree importances extracted via `feature_importances_`
4. **Per-prediction importance** using SHAP values (not just global importance)
5. Results returned as ranked list with percentages (sum to 100%)
6. Top 3 features include plain-English descriptions (e.g., "High demand-to-supply ratio (+32%)")
7. Unit tests verify: all importance values sum to 1.0 (within tolerance)

## Tasks / Subtasks

- [x] Task 1: Create explainability module (AC: 1, 2, 3)
  - [x] Create `backend/src/explainability/__init__.py`
  - [x] Create `backend/src/explainability/feature_importance.py`
  - [x] Extract global importances from each model type

- [x] Task 2: Implement SHAP explainer (AC: 4)
  - [x] Create `backend/src/explainability/shap_explainer.py`
  - [x] Initialize SHAP TreeExplainer for XGBoost/RF
  - [x] Initialize SHAP LinearExplainer for Linear Regression
  - [x] Calculate per-prediction SHAP values

- [x] Task 3: Normalize and rank importances (AC: 5)
  - [x] Normalize values to sum to 100%
  - [x] Sort by absolute importance (descending)
  - [x] Handle negative coefficients (show direction)

- [x] Task 4: Generate plain-English descriptions (AC: 6)
  - [x] Create feature name to description mapping
  - [x] Generate contextual descriptions based on value
  - [x] Example: "High demand-to-supply ratio (+32%)"
  - [x] Include direction (positive/negative impact)

- [x] Task 5: Create FeatureImportance schema
  - [x] Define `FeatureContribution` model
  - [x] Define `FeatureImportanceResult` model
  - [x] Include both global and local (SHAP) importances

- [x] Task 6: Write tests (AC: 7)
  - [x] Test importances sum to 1.0 (within 0.01 tolerance)
  - [x] Test top 3 have descriptions
  - [x] Test SHAP values calculated

## Dev Notes

### SHAP Integration
```python
import shap

class ShapExplainer:
    def __init__(self, model, model_type: str):
        if model_type in ["xgboost", "random_forest", "decision_tree"]:
            self.explainer = shap.TreeExplainer(model)
        elif model_type == "linear_regression":
            self.explainer = shap.LinearExplainer(model, X_train)
    
    def explain(self, X: np.ndarray) -> np.ndarray:
        """Get SHAP values for single prediction"""
        return self.explainer.shap_values(X)
```

### FeatureContribution Schema
```python
class FeatureContribution(BaseModel):
    feature_name: str  # "supply_demand_ratio"
    display_name: str  # "Supply/Demand Ratio"
    importance: float  # 0.32 (32%)
    direction: Literal["positive", "negative"]
    description: str  # "High demand-to-supply ratio (+32%)"
    
class FeatureImportanceResult(BaseModel):
    contributions: list[FeatureContribution]
    model_used: str
    explanation_type: Literal["global", "local_shap"]
    top_3_summary: str  # Natural language summary
```

### Feature Description Templates
```python
feature_descriptions = {
    "supply_demand_ratio": {
        "high": "High demand relative to available drivers",
        "low": "Adequate driver supply for current demand"
    },
    "time_of_booking": {
        "Evening": "Peak evening hours",
        "Morning": "Morning commute period",
        "Night": "Late night hours"
    },
    "location_category": {
        "Urban": "Urban city center location",
        "Suburban": "Suburban area",
        "Rural": "Rural location"
    }
}
```

### Generating Top 3 Summary
```python
def generate_summary(contributions: list[FeatureContribution]) -> str:
    top_3 = contributions[:3]
    parts = [f"{c.description}" for c in top_3]
    return f"Price driven by: {', '.join(parts)}"
```

### Testing

- Test file: `backend/tests/unit/test_explainability/test_feature_importance.py`
- Test: `sum(c.importance for c in result.contributions) ≈ 1.0`
- Test: `len(result.contributions) == num_features`
- Test: SHAP values calculated for each model type

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |
| 2025-12-02 | 1.1 | Implementation complete - All ACs satisfied | Dev Agent (James) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4 (via Cursor)

### Debug Log References
- `uv run pytest tests/unit/test_explainability/ -v` - 40 tests passed
- `uv run pytest -v` - 204 tests passed (full regression)
- No lint errors in new files

### Completion Notes List
- Created explainability module with three main components:
  - `feature_importance.py`: Global importance extraction from model attributes
  - `shap_explainer.py`: Per-prediction SHAP-based importance 
  - `importance_service.py`: Unified service combining normalization, ranking, descriptions
- Created `FeatureContribution` and `FeatureImportanceResult` Pydantic schemas
- All importance values normalized to sum to 1.0 (within 0.01 tolerance)
- Descriptions include percentage and direction (e.g., "+32%")
- Supports all three model types: xgboost, decision_tree, linear_regression
- LinearExplainer requires background_data parameter for SHAP

### File List
**New Files:**
- `backend/src/explainability/__init__.py`
- `backend/src/explainability/feature_importance.py`
- `backend/src/explainability/shap_explainer.py`
- `backend/src/explainability/importance_service.py`
- `backend/src/schemas/explainability.py`
- `backend/tests/unit/test_explainability/__init__.py`
- `backend/tests/unit/test_explainability/test_feature_importance.py`
- `backend/tests/unit/test_explainability/test_shap_explainer.py`
- `backend/tests/unit/test_explainability/test_importance_service.py`

**Modified Files:**
- `backend/src/schemas/__init__.py` (added explainability exports)

## QA Results

### Review Date: 2025-12-02

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent** — Clean, modular implementation with proper separation of concerns.

The implementation follows a well-structured layered approach:
- `feature_importance.py`: Global importance extraction from model attributes
- `shap_explainer.py`: Per-prediction SHAP-based importance with lazy initialization
- `importance_service.py`: Unified service combining normalization, ranking, and descriptions

**Strengths:**
- Proper type hints throughout with `Literal` types for model variants
- Comprehensive docstrings explaining purpose and behavior
- Edge case handling (all-zero coefficients, missing background data)
- Lazy SHAP explainer initialization prevents unnecessary computation
- Good use of Pydantic models with field validation (`ge=0, le=1`)

### Refactoring Performed

None required — code quality is production-ready.

### Compliance Check

- Coding Standards: ✓ Follows Python conventions, proper snake_case, type hints
- Project Structure: ✓ `explainability/` module correctly placed per unified-project-structure.md
- Testing Strategy: ✓ 40 unit tests following testing pyramid (tests/unit/test_explainability/)
- All ACs Met: ✓ All 7 acceptance criteria verified with test coverage

### Requirements Traceability

| AC | Description | Test Coverage | Status |
|----|-------------|---------------|--------|
| AC1 | RF/XGBoost feature_importances_ | test_xgboost_importance_sums_to_one, test_importance_extraction_all_model_types | ✓ FULL |
| AC2 | Linear Regression normalized coef | test_linear_regression_importance_sums_to_one, test_linear_raw_coefficients | ✓ FULL |
| AC3 | Decision Tree feature_importances_ | test_decision_tree_importance_sums_to_one | ✓ FULL |
| AC4 | Per-prediction SHAP values | test_*_shap_values_calculated (3 tests), test_shap_calculated_all_model_types | ✓ FULL |
| AC5 | Ranked list summing to 100% | test_normalize_importance_sums_to_one, test_rank_contributions_descending, test_contributions_are_ranked | ✓ FULL |
| AC6 | Top 3 plain-English descriptions | test_description_includes_percentage, test_top_3_have_descriptions, test_top_3_summary_generated | ✓ FULL |
| AC7 | Sum to 1.0 within tolerance | 5 tests verify `abs(total - 1.0) < 0.01` | ✓ FULL |

### Improvements Checklist

- [x] All acceptance criteria implemented
- [x] Comprehensive test coverage (40 tests)
- [x] Proper error handling for edge cases
- [x] Type hints and documentation complete
- [ ] Consider extracting `ModelType` to shared types module (minor DRY improvement)
- [ ] Consider SHAP computation caching for repeated identical inputs (future optimization)

### Security Review

No security concerns — internal ML computation module with no external input handling.

### Performance Considerations

- SHAP explainer uses lazy initialization (only created when first needed)
- Numpy vectorized operations for normalization
- LinearExplainer correctly requires background_data parameter
- No performance issues identified for expected usage patterns

### Files Modified During Review

None — no refactoring was necessary.

### Gate Status

Gate: PASS → docs/qa/gates/3.1-feature-importance-shap.yml

### Recommended Status

✓ **Ready for Done** — All acceptance criteria met, comprehensive test coverage, clean implementation.

