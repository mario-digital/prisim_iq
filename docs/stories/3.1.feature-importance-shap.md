# Story 3.1: Feature Importance Calculation

## Status: Draft

## Assigned To: Mario

## Story

**As a** pricing analyst,
**I want** to see which factors most influenced the recommendation,
**so that** I can understand and explain decisions to stakeholders.

## Acceptance Criteria

1. Feature importance extracted from Random Forest/XGBoost `feature_importances_` attribute
2. Linear Regression coefficients normalized to percentage scale
3. Decision Tree importances extracted via `feature_importances_`
4. **Per-prediction importance** using SHAP values (not just global importance)
5. Results returned as ranked list with percentages (sum to 100%)
6. Top 3 features include plain-English descriptions (e.g., "High demand-to-supply ratio (+32%)")
7. Unit tests verify: all importance values sum to 1.0 (within tolerance)

## Tasks / Subtasks

- [ ] Task 1: Create explainability module (AC: 1, 2, 3)
  - [ ] Create `backend/src/explainability/__init__.py`
  - [ ] Create `backend/src/explainability/feature_importance.py`
  - [ ] Extract global importances from each model type

- [ ] Task 2: Implement SHAP explainer (AC: 4)
  - [ ] Create `backend/src/explainability/shap_explainer.py`
  - [ ] Initialize SHAP TreeExplainer for XGBoost/RF
  - [ ] Initialize SHAP LinearExplainer for Linear Regression
  - [ ] Calculate per-prediction SHAP values

- [ ] Task 3: Normalize and rank importances (AC: 5)
  - [ ] Normalize values to sum to 100%
  - [ ] Sort by absolute importance (descending)
  - [ ] Handle negative coefficients (show direction)

- [ ] Task 4: Generate plain-English descriptions (AC: 6)
  - [ ] Create feature name to description mapping
  - [ ] Generate contextual descriptions based on value
  - [ ] Example: "High demand-to-supply ratio (+32%)"
  - [ ] Include direction (positive/negative impact)

- [ ] Task 5: Create FeatureImportance schema
  - [ ] Define `FeatureContribution` model
  - [ ] Define `FeatureImportanceResult` model
  - [ ] Include both global and local (SHAP) importances

- [ ] Task 6: Write tests (AC: 7)
  - [ ] Test importances sum to 1.0 (within 0.01 tolerance)
  - [ ] Test top 3 have descriptions
  - [ ] Test SHAP values calculated

## Dev Notes

### SHAP Integration
```python
import shap

class ShapExplainer:
    def __init__(self, model, model_type: str):
        if model_type in ["xgboost", "random_forest", "decision_tree"]:
            self.explainer = shap.TreeExplainer(model)
        elif model_type == "linear_regression":
            self.explainer = shap.LinearExplainer(model, X_train)
    
    def explain(self, X: np.ndarray) -> np.ndarray:
        """Get SHAP values for single prediction"""
        return self.explainer.shap_values(X)
```

### FeatureContribution Schema
```python
class FeatureContribution(BaseModel):
    feature_name: str  # "supply_demand_ratio"
    display_name: str  # "Supply/Demand Ratio"
    importance: float  # 0.32 (32%)
    direction: Literal["positive", "negative"]
    description: str  # "High demand-to-supply ratio (+32%)"
    
class FeatureImportanceResult(BaseModel):
    contributions: list[FeatureContribution]
    model_used: str
    explanation_type: Literal["global", "local_shap"]
    top_3_summary: str  # Natural language summary
```

### Feature Description Templates
```python
feature_descriptions = {
    "supply_demand_ratio": {
        "high": "High demand relative to available drivers",
        "low": "Adequate driver supply for current demand"
    },
    "time_of_booking": {
        "Evening": "Peak evening hours",
        "Morning": "Morning commute period",
        "Night": "Late night hours"
    },
    "location_category": {
        "Urban": "Urban city center location",
        "Suburban": "Suburban area",
        "Rural": "Rural location"
    }
}
```

### Generating Top 3 Summary
```python
def generate_summary(contributions: list[FeatureContribution]) -> str:
    top_3 = contributions[:3]
    parts = [f"{c.description}" for c in top_3]
    return f"Price driven by: {', '.join(parts)}"
```

### Testing

- Test file: `backend/tests/unit/test_explainability/test_feature_importance.py`
- Test: `sum(c.importance for c in result.contributions) â‰ˆ 1.0`
- Test: `len(result.contributions) == num_features`
- Test: SHAP values calculated for each model type

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results
(To be filled by QA Agent)

