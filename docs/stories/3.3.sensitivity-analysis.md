# Story 3.3: Sensitivity Analysis Engine

## Status: Done

## Assigned To: Mario

## Story

**As a** pricing analyst,
**I want** to see how recommendations change under different assumptions,
**so that** I can assess robustness and communicate uncertainty.

## Acceptance Criteria

1. **Elasticity sensitivity:** Recalculate at ±10%, ±20%, ±30% elasticity
2. **Demand sensitivity:** Recalculate at ±10%, ±20% demand modifier
3. **Cost sensitivity:** Recalculate at ±5%, ±10% cost changes
4. Results returned as visualization-ready arrays (price vs. scenario)
5. Confidence band calculated (min/max recommended price across scenarios)
6. Worst-case and best-case scenarios highlighted in response

## Tasks / Subtasks

- [x] Task 1: Create sensitivity service (AC: 1, 2, 3)
  - [x] Create `backend/src/services/sensitivity_service.py`
  - [x] Create `SensitivityService` class
  - [x] Define scenario configurations

- [x] Task 2: Implement elasticity sensitivity (AC: 1)
  - [x] Create scenarios: ±10%, ±20%, ±30% elasticity
  - [x] Recalculate optimal price for each
  - [x] Return array of results

- [x] Task 3: Implement demand sensitivity (AC: 2)
  - [x] Create scenarios: ±10%, ±20% demand modifier
  - [x] Apply modifier to base demand
  - [x] Recalculate optimal price

- [x] Task 4: Implement cost sensitivity (AC: 3)
  - [x] Create scenarios: ±5%, ±10% cost changes
  - [x] Adjust historical_cost_of_ride
  - [x] Recalculate optimal price

- [x] Task 5: Run scenarios in parallel (AC: 4)
  - [x] Use asyncio for parallel execution
  - [x] Aggregate results
  - [x] Meet latency target (< 3s total)

- [x] Task 6: Calculate confidence band (AC: 5)
  - [x] Find min price across all scenarios
  - [x] Find max price across all scenarios
  - [x] Calculate robustness score

- [x] Task 7: Identify extreme scenarios (AC: 6)
  - [x] Find worst-case (lowest profit)
  - [x] Find best-case (highest profit)
  - [x] Include in response

- [x] Task 8: Write tests
  - [x] Test all scenario types generated
  - [x] Test confidence band calculation
  - [x] Test parallel execution

## Dev Notes

### Scenario Definitions
```python
SENSITIVITY_SCENARIOS = {
    "elasticity": [
        {"name": "elasticity_-30%", "modifier": 0.7},
        {"name": "elasticity_-20%", "modifier": 0.8},
        {"name": "elasticity_-10%", "modifier": 0.9},
        {"name": "elasticity_base", "modifier": 1.0},
        {"name": "elasticity_+10%", "modifier": 1.1},
        {"name": "elasticity_+20%", "modifier": 1.2},
        {"name": "elasticity_+30%", "modifier": 1.3},
    ],
    "demand": [
        {"name": "demand_-20%", "modifier": 0.8},
        {"name": "demand_-10%", "modifier": 0.9},
        {"name": "demand_base", "modifier": 1.0},
        {"name": "demand_+10%", "modifier": 1.1},
        {"name": "demand_+20%", "modifier": 1.2},
    ],
    "cost": [
        {"name": "cost_-10%", "modifier": 0.9},
        {"name": "cost_-5%", "modifier": 0.95},
        {"name": "cost_base", "modifier": 1.0},
        {"name": "cost_+5%", "modifier": 1.05},
        {"name": "cost_+10%", "modifier": 1.1},
    ]
}
```

### SensitivityResult Schema
```python
class ScenarioResult(BaseModel):
    scenario_name: str
    scenario_type: Literal["elasticity", "demand", "cost"]
    modifier: float
    optimal_price: float
    expected_profit: float
    expected_demand: float

class SensitivityResult(BaseModel):
    base_price: float
    base_profit: float
    
    elasticity_sensitivity: list[ScenarioResult]
    demand_sensitivity: list[ScenarioResult]
    cost_sensitivity: list[ScenarioResult]
    
    confidence_band: ConfidenceBand
    worst_case: ScenarioResult
    best_case: ScenarioResult
    robustness_score: float  # 0-100
    
class ConfidenceBand(BaseModel):
    min_price: float
    max_price: float
    price_range: float
    range_percent: float  # (max-min)/base * 100
```

### Parallel Execution
```python
async def run_sensitivity_analysis(
    context: MarketContext
) -> SensitivityResult:
    tasks = []
    
    for scenario in SENSITIVITY_SCENARIOS["elasticity"]:
        tasks.append(run_scenario(context, "elasticity", scenario))
    
    for scenario in SENSITIVITY_SCENARIOS["demand"]:
        tasks.append(run_scenario(context, "demand", scenario))
    
    for scenario in SENSITIVITY_SCENARIOS["cost"]:
        tasks.append(run_scenario(context, "cost", scenario))
    
    results = await asyncio.gather(*tasks)
    return aggregate_results(results)
```

### Robustness Score Calculation
```python
def calculate_robustness_score(
    confidence_band: ConfidenceBand,
    base_price: float
) -> float:
    """
    Higher score = more robust (less variation)
    100 = all scenarios give same price
    0 = extreme variation
    """
    range_percent = confidence_band.range_percent
    # Inverse relationship: smaller range = higher score
    return max(0, 100 - range_percent * 2)
```

### Testing

- Test file: `backend/tests/unit/test_services/test_sensitivity_service.py`
- Test 17 total scenarios generated (7 + 5 + 5)
- Test confidence band min < max
- Test robustness score in [0, 100]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |
| 2025-12-02 | 1.1 | Implemented sensitivity service with schemas, parallel execution, tests | Dev Agent (James) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (via Cursor)

### Debug Log References
- `ruff check src/ tests/` - All checks passed
- `pytest tests/unit/test_services/test_sensitivity_service.py -v` - 33 passed
- `pytest tests/ -v` - 197 passed (full regression)

### Completion Notes List
1. Created `backend/src/schemas/sensitivity.py` with `ScenarioResult`, `ConfidenceBand`, and `SensitivityResult` Pydantic models
2. Created `backend/src/services/` directory as new service layer
3. Implemented `SensitivityService` class with:
   - `SENSITIVITY_SCENARIOS` configuration (17 total: 7 elasticity, 5 demand, 5 cost)
   - `run_sensitivity_analysis()` async method with parallel execution via `asyncio.gather()`
   - `_apply_scenario_modifier()` for creating modified MarketContext
   - `_calculate_confidence_band()` for price range analysis
   - `_calculate_robustness_score()` with inverse relationship formula
4. Fixed issue with computed field `supply_demand_ratio` in MarketContext by excluding it from model_dump()
5. All 33 new tests pass, 197 total tests pass (no regression)

### File List
| File | Status |
|------|--------|
| `backend/src/schemas/sensitivity.py` | Created |
| `backend/src/schemas/__init__.py` | Modified |
| `backend/src/services/__init__.py` | Created |
| `backend/src/services/sensitivity_service.py` | Created |
| `backend/tests/unit/test_services/__init__.py` | Created |
| `backend/tests/unit/test_services/test_sensitivity_service.py` | Created |

## QA Results

### Review Date: 2025-12-02

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: EXCELLENT** - Clean, well-structured implementation following all project standards. The sensitivity service demonstrates proper async patterns, clean separation between schemas and business logic, comprehensive test coverage, and good observability through logging.

### Refactoring Performed

None required - code quality is high.

### Compliance Check

- Coding Standards: ✓ Follows 17.3 service structure pattern
- Project Structure: ✓ New services layer properly initialized with `__init__.py`
- Testing Strategy: ✓ Unit tests follow pytest conventions with proper async handling
- All ACs Met: ✓ All 6 acceptance criteria fully implemented and tested

### Improvements Checklist

All items already handled by dev:

- [x] Schema validation with Pydantic Field constraints (ge/le bounds)
- [x] Proper async/await patterns with asyncio.gather()
- [x] Comprehensive test coverage (33 tests)
- [x] Logging for observability (loguru)
- [x] Singleton factory pattern for service instantiation
- [x] Clean exports in schemas/__init__.py and services/__init__.py

### Security Review

No security concerns - pure computation service with no external API calls, user input handling, or auth changes.

### Performance Considerations

- Parallel execution via asyncio.gather() for 17 scenarios
- Latency target (<3s) validated in tests
- Analysis timing tracked via analysis_time_ms field

### Files Modified During Review

None - no refactoring was necessary.

### Gate Status

Gate: PASS → docs/qa/gates/3.3-sensitivity-analysis.yml

### Recommended Status

✓ Ready for Done

