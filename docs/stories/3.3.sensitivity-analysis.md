# Story 3.3: Sensitivity Analysis Engine

## Status: Draft

## Assigned To: Mario

## Story

**As a** pricing analyst,
**I want** to see how recommendations change under different assumptions,
**so that** I can assess robustness and communicate uncertainty.

## Acceptance Criteria

1. **Elasticity sensitivity:** Recalculate at ±10%, ±20%, ±30% elasticity
2. **Demand sensitivity:** Recalculate at ±10%, ±20% demand modifier
3. **Cost sensitivity:** Recalculate at ±5%, ±10% cost changes
4. Results returned as visualization-ready arrays (price vs. scenario)
5. Confidence band calculated (min/max recommended price across scenarios)
6. Worst-case and best-case scenarios highlighted in response

## Tasks / Subtasks

- [ ] Task 1: Create sensitivity service (AC: 1, 2, 3)
  - [ ] Create `backend/src/services/sensitivity_service.py`
  - [ ] Create `SensitivityService` class
  - [ ] Define scenario configurations

- [ ] Task 2: Implement elasticity sensitivity (AC: 1)
  - [ ] Create scenarios: ±10%, ±20%, ±30% elasticity
  - [ ] Recalculate optimal price for each
  - [ ] Return array of results

- [ ] Task 3: Implement demand sensitivity (AC: 2)
  - [ ] Create scenarios: ±10%, ±20% demand modifier
  - [ ] Apply modifier to base demand
  - [ ] Recalculate optimal price

- [ ] Task 4: Implement cost sensitivity (AC: 3)
  - [ ] Create scenarios: ±5%, ±10% cost changes
  - [ ] Adjust historical_cost_of_ride
  - [ ] Recalculate optimal price

- [ ] Task 5: Run scenarios in parallel (AC: 4)
  - [ ] Use asyncio for parallel execution
  - [ ] Aggregate results
  - [ ] Meet latency target (< 3s total)

- [ ] Task 6: Calculate confidence band (AC: 5)
  - [ ] Find min price across all scenarios
  - [ ] Find max price across all scenarios
  - [ ] Calculate robustness score

- [ ] Task 7: Identify extreme scenarios (AC: 6)
  - [ ] Find worst-case (lowest profit)
  - [ ] Find best-case (highest profit)
  - [ ] Include in response

- [ ] Task 8: Write tests
  - [ ] Test all scenario types generated
  - [ ] Test confidence band calculation
  - [ ] Test parallel execution

## Dev Notes

### Scenario Definitions
```python
SENSITIVITY_SCENARIOS = {
    "elasticity": [
        {"name": "elasticity_-30%", "modifier": 0.7},
        {"name": "elasticity_-20%", "modifier": 0.8},
        {"name": "elasticity_-10%", "modifier": 0.9},
        {"name": "elasticity_base", "modifier": 1.0},
        {"name": "elasticity_+10%", "modifier": 1.1},
        {"name": "elasticity_+20%", "modifier": 1.2},
        {"name": "elasticity_+30%", "modifier": 1.3},
    ],
    "demand": [
        {"name": "demand_-20%", "modifier": 0.8},
        {"name": "demand_-10%", "modifier": 0.9},
        {"name": "demand_base", "modifier": 1.0},
        {"name": "demand_+10%", "modifier": 1.1},
        {"name": "demand_+20%", "modifier": 1.2},
    ],
    "cost": [
        {"name": "cost_-10%", "modifier": 0.9},
        {"name": "cost_-5%", "modifier": 0.95},
        {"name": "cost_base", "modifier": 1.0},
        {"name": "cost_+5%", "modifier": 1.05},
        {"name": "cost_+10%", "modifier": 1.1},
    ]
}
```

### SensitivityResult Schema
```python
class ScenarioResult(BaseModel):
    scenario_name: str
    scenario_type: Literal["elasticity", "demand", "cost"]
    modifier: float
    optimal_price: float
    expected_profit: float
    expected_demand: float

class SensitivityResult(BaseModel):
    base_price: float
    base_profit: float
    
    elasticity_sensitivity: list[ScenarioResult]
    demand_sensitivity: list[ScenarioResult]
    cost_sensitivity: list[ScenarioResult]
    
    confidence_band: ConfidenceBand
    worst_case: ScenarioResult
    best_case: ScenarioResult
    robustness_score: float  # 0-100
    
class ConfidenceBand(BaseModel):
    min_price: float
    max_price: float
    price_range: float
    range_percent: float  # (max-min)/base * 100
```

### Parallel Execution
```python
async def run_sensitivity_analysis(
    context: MarketContext
) -> SensitivityResult:
    tasks = []
    
    for scenario in SENSITIVITY_SCENARIOS["elasticity"]:
        tasks.append(run_scenario(context, "elasticity", scenario))
    
    for scenario in SENSITIVITY_SCENARIOS["demand"]:
        tasks.append(run_scenario(context, "demand", scenario))
    
    for scenario in SENSITIVITY_SCENARIOS["cost"]:
        tasks.append(run_scenario(context, "cost", scenario))
    
    results = await asyncio.gather(*tasks)
    return aggregate_results(results)
```

### Robustness Score Calculation
```python
def calculate_robustness_score(
    confidence_band: ConfidenceBand,
    base_price: float
) -> float:
    """
    Higher score = more robust (less variation)
    100 = all scenarios give same price
    0 = extreme variation
    """
    range_percent = confidence_band.range_percent
    # Inverse relationship: smaller range = higher score
    return max(0, 100 - range_percent * 2)
```

### Testing

- Test file: `backend/tests/unit/test_services/test_sensitivity_service.py`
- Test 17 total scenarios generated (7 + 5 + 5)
- Test confidence band min < max
- Test robustness score in [0, 100]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results
(To be filled by QA Agent)

