# Story 3.5: Explain Decision Endpoint

## Status: Ready for Review

## Assigned To: Mario

## Story

**As a** frontend developer,
**I want** an API endpoint returning full explanation,
**so that** UI can display "why" information alongside recommendations.

## Acceptance Criteria

1. `POST /api/v1/explain_decision` endpoint accepts MarketContext + optional pricing result reference
2. Returns: `recommendation`, `feature_importance[]`, `decision_trace`, `natural_language_summary`, `model_agreement`
3. Natural language summary generated (e.g., "The recommended price of $42.50 is primarily driven by high demand in your urban segment during evening hours...")
4. Response time < 2 seconds
5. Documented in Swagger with comprehensive examples

## Tasks / Subtasks

- [x] Task 1: Create explanation router (AC: 1)
  - [x] Add to `backend/src/api/routers/explain.py`
  - [x] Register with `/api/v1` prefix
  - [x] Define request schema

- [x] Task 2: Create PriceExplanation schema (AC: 2)
  - [x] Create `backend/src/schemas/explanation.py`
  - [x] Include all required fields
  - [x] Add Swagger examples

- [x] Task 3: Implement explanation service (AC: 2)
  - [x] Create `backend/src/services/explanation_service.py`
  - [x] Orchestrate feature importance calculation
  - [x] Generate decision trace
  - [x] Calculate model agreement

- [x] Task 4: Implement natural language generation (AC: 3)
  - [x] Template-based narrative generation
  - [x] Include top factors
  - [x] Contextual descriptions

- [x] Task 5: Implement endpoint (AC: 1, 4)
  - [x] POST /api/v1/explain_decision
  - [x] Accept MarketContext
  - [x] Optionally accept previous pricing_result_id
  - [x] Ensure < 2 second response

- [x] Task 6: Add Swagger documentation (AC: 5)
  - [x] Detailed descriptions
  - [x] Full example request/response
  - [x] Error documentation

- [x] Task 7: Write tests
  - [x] Test valid explanation returned
  - [x] Test natural language summary generated
  - [x] Test response time < 2 seconds

## Dev Notes

### PriceExplanation Schema
```python
class ExplainRequest(BaseModel):
    context: MarketContext
    pricing_result_id: str | None = None  # Reference previous result
    include_trace: bool = True
    include_shap: bool = True
    
class PriceExplanation(BaseModel):
    # Core recommendation (may be recalculated or referenced)
    recommendation: PricingResult
    
    # Feature importance
    feature_importance: list[FeatureContribution]
    global_importance: list[FeatureContribution]  # Model-level
    
    # Decision trace
    decision_trace: DecisionTrace
    
    # Model comparison
    model_agreement: ModelAgreement
    model_predictions: dict[str, float]
    
    # Natural language
    natural_language_summary: str
    key_factors: list[str]  # ["High demand", "Evening peak", "Premium vehicle"]
    
    # Metadata
    explanation_time_ms: float
    timestamp: datetime
```

### Natural Language Generation
```python
def generate_narrative(
    result: PricingResult,
    importance: list[FeatureContribution],
    context: MarketContext
) -> str:
    """
    Generate human-readable explanation.
    Template-based, NOT LLM-generated (for speed).
    """
    top_factors = importance[:3]
    
    narrative = f"The recommended price of ${result.recommended_price:.2f} "
    
    # Add primary driver
    primary = top_factors[0]
    narrative += f"is primarily driven by {primary.description.lower()} "
    narrative += f"(contributing {primary.importance*100:.0f}% to the decision). "
    
    # Add secondary factors
    if len(top_factors) > 1:
        secondary = [f.display_name.lower() for f in top_factors[1:]]
        narrative += f"Additional factors include {' and '.join(secondary)}. "
    
    # Add profit context
    narrative += f"This price is expected to generate ${result.expected_profit:.2f} "
    narrative += f"in profit, a {result.profit_uplift_percent:.1f}% improvement "
    narrative += "over the baseline price."
    
    return narrative

# Example output:
# "The recommended price of $42.50 is primarily driven by high demand-to-supply 
# ratio (contributing 32% to the decision). Additional factors include evening 
# peak hours and premium vehicle selection. This price is expected to generate 
# $18.75 in profit, a 24.3% improvement over the baseline price."
```

### Endpoint Implementation
```python
@router.post("/explain_decision", response_model=PriceExplanation)
async def explain_decision(
    request: ExplainRequest,
    explanation_service: ExplanationService = Depends(get_explanation_service)
) -> PriceExplanation:
    return await explanation_service.explain(request)
```

### Performance Target
- Total response time < 2 seconds
- Feature importance: < 500ms
- Decision trace: < 500ms
- Natural language: < 100ms

### Testing

- Test file: `backend/tests/integration/test_api/test_explain.py`
- Test all fields present in response
- Test natural_language_summary is non-empty string
- Test response time < 2000ms

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |
| 2024-12-03 | 2.0 | Implementation complete - all tasks done, 18 tests passing | Dev Agent (James) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (via Cursor IDE)

### Debug Log References
- Generated training data: `uv run python scripts/generate_training_data.py`
- Trained ML models: `uv run python -m src.ml.training`
- Ran integration tests: `uv run pytest tests/integration/test_api/test_explain.py -v` (18 passed)
- Ran full regression: `uv run pytest tests/ -v` (366 passed, 4 warnings)

### Completion Notes List
1. Created `ExplainRequest` and `PriceExplanation` schemas in `schemas/explanation.py` with full Pydantic validation and Swagger examples
2. Implemented `ExplanationService` in `services/explanation_service.py` that orchestrates:
   - Traced pricing via `TracedPricingService`
   - Global and local (SHAP) feature importance via `FeatureImportanceService`
   - Model agreement calculation across all models (xgboost, decision_tree, linear_regression)
3. Implemented template-based natural language generation (`generate_narrative`) for human-readable explanations
4. Added `extract_key_factors` function for contextual factor summaries
5. Created `/api/v1/explain_decision` router with comprehensive error handling (422, 500, 503)
6. Added detailed Swagger documentation with examples for all response codes
7. Registered router in `main.py` and `routers/__init__.py`
8. Updated `schemas/__init__.py` and `services/__init__.py` exports
9. Created 18 integration tests covering all acceptance criteria

### File List
**New Files:**
- `backend/src/schemas/explanation.py` - ExplainRequest and PriceExplanation schemas
- `backend/src/services/explanation_service.py` - ExplanationService with NL generation
- `backend/src/api/routers/explain.py` - explain_decision endpoint router
- `backend/tests/integration/test_api/test_explain.py` - 18 integration tests

**Modified Files:**
- `backend/src/api/routers/__init__.py` - Added explain router export
- `backend/src/main.py` - Registered explain router with /api/v1 prefix
- `backend/src/schemas/__init__.py` - Added ExplainRequest, PriceExplanation exports
- `backend/src/services/__init__.py` - Added ExplanationService, get_explanation_service exports

## QA Results

### Review Date: 2024-12-03

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent** - The implementation demonstrates strong adherence to project standards with well-structured code, comprehensive documentation, and thorough test coverage.

**Strengths:**
- Clean service layer architecture with proper dependency injection
- Comprehensive Pydantic schemas with full validation and Swagger examples
- Template-based natural language generation (smart choice for < 2s performance)
- Robust error handling with specific exception types (503, 500, 422)
- Excellent test organization with clear test class separation by concern

**Code Patterns Verified:**
- Follows `router + service + schema` pattern established in project ✅
- Proper singleton pattern for service initialization ✅
- Type hints throughout all modules ✅
- Comprehensive docstrings on all public functions ✅
- Logging at appropriate levels (info for requests, warning for fallbacks) ✅

### Refactoring Performed

None required. Implementation is clean and follows project standards.

### Compliance Check

- Coding Standards: ✓ - Follows Python typing, docstrings, Pydantic validation patterns
- Project Structure: ✓ - Files placed in correct locations (`api/routers/`, `schemas/`, `services/`)
- Testing Strategy: ✓ - Integration tests at appropriate level for API endpoint
- All ACs Met: ✓ - All 5 acceptance criteria verified with tests

### Improvements Checklist

All items are optional future improvements (not blocking):

- [ ] Consider adding unit tests for `generate_narrative()` and `extract_key_factors()` functions
- [ ] Future: Replace `datetime.utcnow()` with `datetime.now(timezone.utc)` when upgrading to Python 3.12+
- [ ] Nice-to-have: Add narrative variation templates for more natural-sounding explanations
- [ ] Consider: Add caching for global_importance (doesn't change per request)

### Security Review

**Status: PASS**
- Input validation via Pydantic with `extra="forbid"` prevents payload injection
- No hardcoded secrets or sensitive data exposure
- Error responses sanitized (internal details not leaked to clients)
- Proper HTTP status codes (422 for validation, 503 for service unavailable)

### Performance Considerations

**Status: PASS**
- Performance target of < 2 seconds is met (verified via `test_response_time_under_2_seconds`)
- Lazy singleton initialization prevents startup delay
- Template-based NL generation instead of LLM (smart performance choice)
- `explanation_time_ms` field provides observability for monitoring

### Files Modified During Review

None. No refactoring was required.

### Test Architecture Assessment

**18 Integration Tests - Well Organized:**
- `TestExplainDecisionEndpoint` (6 tests) - Core functionality
- `TestExplainDecisionOptions` (2 tests) - Request options
- `TestExplainDecisionValidation` (3 tests) - Input validation
- `TestExplainDecisionPerformance` (2 tests) - Performance AC
- `TestExplainDecisionSwagger` (4 tests) - Documentation AC
- `TestExplainDecisionConsistency` (1 test) - Output stability

**Traceability Matrix:**
| AC | Description | Tests |
|----|-------------|-------|
| AC1 | POST endpoint accepts MarketContext | test_explain_decision_returns_valid_response, test_missing_context |
| AC2 | Returns all required fields | test_*_structure (4 tests) |
| AC3 | Natural language summary | test_natural_language_summary_generated, test_key_factors_populated |
| AC4 | Response < 2 seconds | test_response_time_under_2_seconds, test_explanation_time_reported |
| AC5 | Swagger documentation | test_openapi_* (4 tests) |

### Gate Status

Gate: PASS → docs/qa/gates/3.5-explain-decision-endpoint.yml

### Recommended Status

✓ **Ready for Done**

All acceptance criteria are met, 18 integration tests pass, 366 total tests pass (no regression), code follows project standards, and performance target is verified.

---

## Story DoD Checklist Results

### 1. Requirements Met ✅
- [x] All functional requirements specified in the story are implemented
  - POST /api/v1/explain_decision endpoint created
  - Returns recommendation, feature_importance[], decision_trace, natural_language_summary, model_agreement
- [x] All acceptance criteria defined in the story are met
  - AC1: Endpoint accepts MarketContext + optional pricing_result_id ✅
  - AC2: Returns all required fields ✅
  - AC3: Natural language summary generated with top factors ✅
  - AC4: Response time < 2 seconds (verified via tests) ✅
  - AC5: Documented in Swagger with comprehensive examples ✅

### 2. Coding Standards & Project Structure ✅
- [x] Code adheres to Operational Guidelines (Python typing, docstrings, Pydantic validation)
- [x] Files placed in correct locations per Project Structure
- [x] Uses approved tech stack (FastAPI, Pydantic, SHAP)
- [x] API follows existing patterns (router + service + schema)
- [x] Basic security (input validation via Pydantic, proper error handling)
- [x] No new linter errors or warnings
- [x] Code is well-commented with docstrings

### 3. Testing ✅
- [x] 18 integration tests implemented covering all ACs
- [x] All 366 tests pass (including new tests)
- [x] Test coverage includes validation, performance, and Swagger tests

### 4. Functionality & Verification ✅
- [x] Functionality verified via test suite execution
- [x] Edge cases handled (invalid input returns 422, missing models returns 503)

### 5. Story Administration ✅
- [x] All tasks marked complete
- [x] Implementation decisions documented in Completion Notes
- [x] Change Log updated

### 6. Dependencies, Build & Configuration ✅
- [x] Project builds successfully (imports work)
- [x] Linting passes (0 errors)
- [x] No new dependencies added (uses existing SHAP, sklearn)
- [x] No new environment variables needed

### 7. Documentation ✅
- [x] All new functions have comprehensive docstrings
- [x] Swagger documentation complete with examples
- [x] File List documents all changes

### Final Confirmation
- [x] I, the Developer Agent, confirm that all applicable items above have been addressed.

