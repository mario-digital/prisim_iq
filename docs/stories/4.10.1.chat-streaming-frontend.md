# Story 4.10.1: Chat Streaming Frontend Integration

## Status: Draft

## Assigned To: Pending

## Story

**As a** pricing analyst,
**I want** to see AI responses appear token-by-token in real-time,
**so that** I have immediate feedback that the system is working and can read responses as they generate.

## Acceptance Criteria

1. Chat service updated to handle SSE streaming from `/api/v1/chat`
2. Messages render progressively as tokens arrive (typewriter effect)
3. Tool call indicators shown when agent invokes backend tools
4. Typing indicator replaced by actual streaming content
5. Error handling for stream interruptions with retry option
6. Smooth transition from streaming to complete message
7. Non-streaming fallback if SSE connection fails

## Tasks / Subtasks

- [ ] Task 1: Update chat service for SSE (AC: 1)
  - [ ] Modify `frontend/src/services/chatService.ts`
  - [ ] Implement `EventSource` or `fetch` with ReadableStream
  - [ ] Parse SSE `data:` events
  - [ ] Handle connection lifecycle (open, message, error, close)

- [ ] Task 2: Create streaming message state (AC: 2, 6)
  - [ ] Update `frontend/src/stores/chatStore.ts`
  - [ ] Add `streamingMessage: string | null` state
  - [ ] Add `appendToStreaming(token: string)` action
  - [ ] Add `finalizeStreaming()` action to convert to complete message

- [ ] Task 3: Update AIMessage for streaming (AC: 2)
  - [ ] Modify `frontend/src/components/chat/AIMessage.tsx`
  - [ ] Accept `isStreaming` prop
  - [ ] Render partial content with cursor indicator
  - [ ] Smooth CSS transition for new tokens

- [ ] Task 4: Create tool call indicator (AC: 3)
  - [ ] Create `frontend/src/components/chat/ToolCallIndicator.tsx`
  - [ ] Display tool name being called (e.g., "üîß Calling optimize_price...")
  - [ ] Animate while tool executes
  - [ ] Collapse/fade when tool completes

- [ ] Task 5: Update ChatPanel orchestration (AC: 2, 4, 6)
  - [ ] Modify `frontend/src/components/chat/ChatPanel.tsx`
  - [ ] Replace typing indicator with streaming message when tokens arrive
  - [ ] Show ToolCallIndicator during tool execution
  - [ ] Handle stream completion

- [ ] Task 6: Implement error handling (AC: 5, 7)
  - [ ] Detect stream disconnection
  - [ ] Show "Connection lost" message with retry button
  - [ ] Implement retry logic with exponential backoff
  - [ ] Fall back to non-streaming if SSE fails 3 times

- [ ] Task 7: Add streaming visual polish (AC: 2, 6)
  - [ ] Add blinking cursor at end of streaming text
  - [ ] Smooth scroll as content grows
  - [ ] Fade out cursor on completion
  - [ ] Match typing speed to token arrival

- [ ] Task 8: Write tests
  - [ ] Test SSE parsing
  - [ ] Test progressive rendering
  - [ ] Test error recovery
  - [ ] Test store state transitions

## Dev Notes

### SSE Event Format (from Backend 4.9.1)
```
data: {"token": "The ", "done": false}

data: {"token": "optimal ", "done": false}

data: {"tool_call": "optimize_price", "done": false}

data: {"token": "price is $24.50", "done": false}

data: {"message": "...", "tools_used": ["optimize_price"], "done": true}

```

### Updated Chat Service
```typescript
// services/chatService.ts
export async function* streamMessage(
  message: string,
  context: MarketContext
): AsyncGenerator<ChatStreamEvent> {
  const response = await fetch(`${API_URL}/api/v1/chat?stream=true`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message, context }),
  });

  const reader = response.body?.getReader();
  const decoder = new TextDecoder();

  if (!reader) throw new Error('No response body');

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n');

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = JSON.parse(line.slice(6));
        yield data as ChatStreamEvent;
        if (data.done) return;
      }
    }
  }
}
```

### Updated Chat Store
```typescript
// stores/chatStore.ts
interface ChatState {
  messages: Message[];
  isLoading: boolean;
  streamingContent: string | null;
  currentToolCall: string | null;
  // ... existing
  
  startStreaming: () => void;
  appendToken: (token: string) => void;
  setToolCall: (toolName: string | null) => void;
  finalizeStream: (fullMessage: Message) => void;
}
```

### Streaming AIMessage
```tsx
// AIMessage.tsx updates
interface AIMessageProps {
  message: Message;
  isStreaming?: boolean;
}

export function AIMessage({ message, isStreaming }: AIMessageProps) {
  return (
    <div className="flex justify-start">
      <div className="max-w-[80%] bg-muted rounded-2xl rounded-tl-sm px-4 py-2">
        <ReactMarkdown>{message.content}</ReactMarkdown>
        {isStreaming && (
          <span className="inline-block w-2 h-4 bg-primary animate-pulse ml-1" />
        )}
        {!isStreaming && message.confidence && (
          <ConfidenceBadge value={message.confidence} />
        )}
      </div>
    </div>
  );
}
```

### Tool Call Indicator
```tsx
// ToolCallIndicator.tsx
export function ToolCallIndicator({ toolName }: { toolName: string }) {
  const toolLabels: Record<string, string> = {
    optimize_price: 'üí∞ Calculating optimal price...',
    explain_decision: 'üîç Analyzing decision factors...',
    sensitivity_analysis: 'üìä Running sensitivity analysis...',
    get_segment: 'üì¶ Identifying segment...',
  };

  return (
    <div className="flex items-center gap-2 text-sm text-muted-foreground py-2">
      <div className="w-4 h-4 border-2 border-primary border-t-transparent rounded-full animate-spin" />
      <span>{toolLabels[toolName] || `üîß Calling ${toolName}...`}</span>
    </div>
  );
}
```

### ChatPanel Streaming Integration
```tsx
// ChatPanel.tsx - streaming flow
const handleSend = async (message: string) => {
  addMessage({ role: 'user', content: message });
  startStreaming();
  
  try {
    for await (const event of streamMessage(message, context)) {
      if (event.token) {
        appendToken(event.token);
      }
      if (event.tool_call) {
        setToolCall(event.tool_call);
      }
      if (event.done) {
        finalizeStream({
          role: 'assistant',
          content: event.message,
          confidence: event.confidence,
          toolsUsed: event.tools_used,
        });
      }
    }
  } catch (error) {
    // Handle error, offer retry
  }
};
```

### Testing

- Test file: `frontend/tests/unit/chat/streaming.test.tsx`
- Mock SSE responses
- Test token accumulation
- Test error recovery

## Dependencies

- **Requires Story 4.9.1** (Backend LangChain with Streaming) to be complete
- Built on top of Story 4.3 (Center Panel Chat) components

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation - Frontend streaming integration | SM Agent (Bob) |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results
(To be filled by QA Agent)

