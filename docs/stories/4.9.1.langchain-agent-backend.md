# Story 4.9.1: LangChain Agent Backend with Streaming

## Status: Ready for Review

## Assigned To: Mario

## Story

**As a** frontend developer,
**I want** a backend chat endpoint powered by LangChain with streaming support,
**so that** users see real-time token-by-token responses in the chat interface.

## Acceptance Criteria

1. LangChain agent implemented in `backend/src/agent/agent.py` with all required tools
2. **Tools registered:**
   - `optimize_price`: Get optimal price for context
   - `explain_decision`: Get full explanation
   - `sensitivity_analysis`: Get sensitivity data
   - `get_segment`: Get segment classification
   - `get_eda_summary`: Get dataset statistics
   - `get_external_context`: Get current external factors (stub/mock)
   - `get_evidence`: Get documentation
   - `get_honeywell_mapping`: Get mapping documentation
3. Agent correctly routes natural language queries to appropriate tools
4. Agent generates natural language responses incorporating tool results
5. Agent maintains conversation context (memory) within session
6. `POST /api/v1/chat` endpoint supports **Server-Sent Events (SSE)** streaming
7. Non-streaming fallback available via query param `?stream=false`
8. Error handling for edge cases (ambiguous queries, tool failures)

## Tasks / Subtasks

- [x] Task 1: Create agent module structure (AC: 1)
  - [x] Create `backend/src/agent/__init__.py`
  - [x] Create `backend/src/agent/agent.py`
  - [x] Create `backend/src/agent/tools/` directory
  - [x] Create `backend/src/agent/prompts/` directory

- [x] Task 2: Create pricing tools (AC: 2)
  - [x] Create `backend/src/agent/tools/__init__.py`
  - [x] Create `backend/src/agent/tools/pricing_tools.py`
  - [x] Define `optimize_price` tool wrapping existing service
  - [x] Define `explain_decision` tool
  - [x] Define `sensitivity_analysis` tool

- [x] Task 3: Create data tools (AC: 2)
  - [x] Create `backend/src/agent/tools/data_tools.py`
  - [x] Define `get_segment` tool wrapping segmenter
  - [x] Define `get_eda_summary` tool wrapping EDA endpoint
  - [x] Define `get_external_context` tool (stub returning mock data)

- [x] Task 4: Create documentation tools (AC: 2)
  - [x] Create `backend/src/agent/tools/doc_tools.py`
  - [x] Define `get_evidence` tool
  - [x] Define `get_honeywell_mapping` tool

- [x] Task 5: Create system prompt (AC: 3, 4)
  - [x] Create `backend/src/agent/prompts/__init__.py`
  - [x] Create `backend/src/agent/prompts/system.py`
  - [x] Define agent persona as "PrismIQ Pricing Copilot"
  - [x] Include tool usage instructions
  - [x] Add response formatting guidelines

- [x] Task 6: Implement agent with LangChain v1 tool-calling (AC: 3, 4, 5)
  - [x] Initialize tool-calling agent via `create_tool_calling_agent`
  - [x] Wrap with `AgentExecutor`
  - [x] Use `ChatPromptTemplate` with `chat_history` placeholder
  - [x] Test tool selection for different query types

- [x] Task 7: Implement streaming (AC: 6)
  - [x] Create `backend/src/agent/streaming.py`
  - [x] Use `AgentExecutor.astream_events(version="v1")` for normalized events
  - [x] Create async generator for SSE events
  - [x] Format as `data: {"token": "...", "done": false}\n\n`

- [x] Task 8: Create chat router with SSE (AC: 6, 7)
  - [x] Create `backend/src/api/routers/chat.py`
  - [x] Implement `POST /api/v1/chat` with `StreamingResponse`
  - [x] Support `?stream=false` for non-streaming fallback
  - [x] Return `Content-Type: text/event-stream` for streaming

- [x] Task 9: Create schemas (AC: 6, 7)
  - [x] Create `backend/src/schemas/chat.py`
  - [x] Define `ChatRequest` (message, context, session_id)
  - [x] Define `ChatResponse` (message, tools_used, pricing_result, timestamp)
  - [x] Define `ChatStreamEvent` (token, done, error)

- [x] Task 10: Add error handling (AC: 8)
  - [x] Handle ambiguous queries with clarification prompt
  - [x] Handle tool failures gracefully with fallback message
  - [x] Return structured error events in stream

- [x] Task 11: Write tests
  - [x] Test tool routing for different query patterns
  - [x] Test memory persistence across calls
  - [x] Test streaming output format
  - [x] Test error handling

## Dev Notes

### Agent Architecture
```
backend/src/agent/
├── __init__.py
├── agent.py              # Main agent setup + PrismIQAgent class
├── streaming.py          # SSE streaming handler
├── tools/
│   ├── __init__.py
│   ├── pricing_tools.py  # optimize_price, explain_decision, sensitivity_analysis
│   ├── data_tools.py     # get_segment, get_eda_summary, get_external_context
│   └── doc_tools.py      # get_evidence, get_honeywell_mapping
└── prompts/
    ├── __init__.py
    └── system.py         # System prompt constant
```

### Dependencies to Add
```python
# pyproject.toml [project.optional-dependencies] agent section
langchain>=1.1.0          # LangChain v1.1 (December 2025) - model profiles, middleware
langchain-core>=1.0.0
langchain-openai>=1.0.0
langgraph>=1.0.0          # LangGraph v1.0 (October 2025) - durable state, persistence
sse-starlette>=1.8.2      # For SSE streaming in FastAPI
openai>=1.50.0
```

### Environment Variable
```bash
OPENAI_API_KEY=sk-...  # Required for LangChain OpenAI
```

### LangChain v1.1 / LangGraph v1.0 Notes
- Use `langchain-openai` package models via `ChatOpenAI` (tool-calling capable models)
- Define tools with `@tool` from `langchain_core.tools` (typed args supported)
- Build agents with `create_tool_calling_agent` and run via `AgentExecutor`
- Maintain per-session memory by passing `chat_history` in the prompt inputs
- **Streaming**: Use `AgentExecutor.astream_events(..., version="v1")` to get normalized events
- **System prompt**: Use `ChatPromptTemplate` with messages: system, placeholder `chat_history`, human
- Use LangGraph when you need multi-agent orchestration or durable checkpoints; not required for single-agent flows
- **LangChain v1.1 features**: Model profiles, middleware, improved structured tool calling
- **LangGraph v1.0 features**: Durable state, persistence, human-in-the-loop patterns
- Migration guide: https://docs.langchain.com/oss/python/migrate/langchain-v1

### SSE Streaming Format
```
data: {"token": "The ", "done": false}

data: {"token": "optimal ", "done": false}

data: {"token": "price ", "done": false}

data: {"tool_call": "optimize_price", "done": false}

data: {"token": "is $24.50", "done": false}

data: {"message": "The optimal price is $24.50...", "tools_used": ["optimize_price"], "done": true}

```

### Tool Definitions (LangChain v1.1 - @tool decorator)
```python
# tools/pricing_tools.py (pattern used in backend)
from langchain_core.tools import tool

@tool
def optimize_price(query: str = "current") -> str:
    """Get the optimal price recommendation for the current market context.

    Use when asked about pricing or recommendations. Context is provided
    implicitly via the request/session and accessed inside the tool.
    """
    from src.agent.context import get_current_context
    from src.services.pricing_service import get_pricing_service
    from src.agent.utils import run_sync
    ctx = get_current_context()
    svc = get_pricing_service()
    res = run_sync(svc.get_recommendation(ctx))
    return f"Optimal Price: ${res.recommended_price:.2f} (Profit: ${res.expected_profit:.2f})"

@tool
def explain_decision(query: str = "current") -> str:
    """Explain a pricing recommendation with key factors and SHAP summary."""
    from src.agent.context import get_current_context
    from src.services.explanation_service import get_explanation_service
    from src.agent.utils import run_sync
    from src.schemas.explanation import ExplainRequest
    ctx = get_current_context()
    svc = get_explanation_service()
    res = run_sync(svc.explain(ExplainRequest(context=ctx, include_shap=True)))
    return res.natural_language_summary

@tool
def sensitivity_analysis(query: str = "current") -> str:
    """Analyze robustness of the recommendation across scenarios."""
    from src.agent.context import get_current_context
    from src.services.sensitivity_service import get_sensitivity_service
    from src.agent.utils import run_sync
    ctx = get_current_context()
    svc = get_sensitivity_service()
    res = run_sync(svc.run_sensitivity_analysis(ctx))
    return f"Band: ${res.confidence_band.min_price:.2f}-${res.confidence_band.max_price:.2f}"
```

### Agent Setup (LangChain v1.1 - tool-calling AgentExecutor)
```python
from typing import Dict, List
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Model and tools
llm = ChatOpenAI(model="gpt-4o", temperature=0)
tools = [optimize_price, explain_decision, sensitivity_analysis, get_segment, get_eda_summary, get_external_context, get_evidence, get_honeywell_mapping]

# Prompt with chat history placeholder
prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("placeholder", "chat_history"),
    ("human", "{input}"),
])

# Agent runnable + executor
agent_runnable = create_tool_calling_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent_runnable, tools=tools)

# Simple per-session memory
history: Dict[str, List[HumanMessage | AIMessage]] = {}

async def chat(session_id: str, message: str) -> str:
    msgs = history.get(session_id, [])
    res = await executor.ainvoke({"input": message, "chat_history": msgs})
    output = res["output"]
    msgs = msgs + [HumanMessage(content=message), AIMessage(content=output)]
    history[session_id] = msgs
    return output

async def stream(session_id: str, message: str):
    msgs = history.get(session_id, [])
    final = ""
    tools_used: List[str] = []
    async for event in executor.astream_events({"input": message, "chat_history": msgs}, version="v1"):
        et = event.get("event")
        if et == "on_llm_new_token":
            token = event.get("data", {}).get("token", "")
            if token:
                final += token
                yield {"token": token, "done": False}
        elif et == "on_tool_start":
            name = event.get("name") or event.get("data", {}).get("name")
            if name and name not in tools_used:
                tools_used.append(name)
                yield {"tool_call": name, "done": False}

    msgs = msgs + [HumanMessage(content=message), AIMessage(content=final)]
    history[session_id] = msgs
    yield {"message": final, "tools_used": tools_used, "done": True}
```

### Streaming SSE Generator
```python
# streaming.py
from typing import AsyncGenerator
import json

async def sse_generator(agent: PrismIQAgent, message: str, context: MarketContext, session_id: str) -> AsyncGenerator[str, None]:
    """Convert agent stream to SSE format."""
    try:
        async for event in agent.stream_chat(message, context, session_id):
            yield f"data: {json.dumps(event)}\n\n"
    except Exception as e:
        yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"
```

### Chat Endpoint with SSE
```python
# api/routers/chat.py
from fastapi import APIRouter, Query
from sse_starlette.sse import EventSourceResponse

router = APIRouter(prefix="/api/v1", tags=["chat"])

@router.post("/chat")
async def chat(
    request: ChatRequest,
    stream: bool = Query(True, description="Enable streaming response")
):
    if stream:
        return EventSourceResponse(
            sse_generator(agent, request.message, request.context, request.session_id or "default"),
            media_type="text/event-stream"
        )
    else:
        result = await agent.chat(request.message, request.context, request.session_id or "default")
        return {"message": result}
```

### Existing Services to Wire Up
- `PriceOptimizer` from `src/ml/price_optimizer.py` → `optimize_price` tool
- `Segmenter` from `src/ml/segmenter.py` → `get_segment` tool
- EDA summary from `data/eda_summary.json` → `get_eda_summary` tool

### Testing

- Test file: `backend/tests/unit/test_agent/`
- Test tool selection: "What's the optimal price?" → should call `optimize_price`
- Test memory: Follow-up question should have context
- Test streaming: Verify SSE format and token flow

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-02 | 1.0 | Initial story creation - Backend LangChain with streaming | SM Agent (Bob) |
| 2025-12-02 | 1.1 | Updated to LangChain v1.0 GA + LangGraph v1.0 (October 2025 release) | SM Agent (Bob) |
| 2025-12-03 | 1.2 | Implemented LangChain v1.1 tool-calling agent with SSE streaming and @tool decorators | Dev Agent (James) |
| 2025-12-03 | 1.3 | Switched to `create_tool_calling_agent` + `AgentExecutor`, streaming via `astream_events(version="v1")` | Dev Agent (James) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (via Cursor)

### Debug Log References
- `ruff check src/agent/ src/schemas/chat.py src/api/routers/chat.py` - All checks passed
- `pytest tests/unit/test_agent/ -v` - 43 tests passed
- Full test suite: 464 passed, 7 pre-existing failures in test_sensitivity_service.py (unrelated to agent work)

### Completion Notes List
1. **Agent Architecture (v1.1)**: Implemented tool-calling agent using `create_tool_calling_agent` and `AgentExecutor`
2. **Tool Decorator Pattern**: All 8 tools use `@tool` from `langchain_core.tools`
3. **Session Memory**: Per-session `chat_history` passed into the prompt (no MemorySaver for single-agent flow)
4. **Streaming (v1)**: Use `AgentExecutor.astream_events(..., version="v1")` for normalized token/tool events
5. **Prompt**: System prompt supplied via `ChatPromptTemplate` with `chat_history` placeholder
6. **Non-Streaming Fallback**: `?stream=false` query param provides traditional JSON response mode
7. **ChatStreamEvent Schema**: New Pydantic schema for SSE event validation and documentation
8. **Dependencies Added**: `langchain>=1.1.0`, `langchain-core>=1.0.0`, `langchain-openai>=1.0.0`, `sse-starlette>=1.8.2`, `openai>=1.50.0` added to pyproject.toml
9. **Error Handling**: Stream errors emit `{"error": "...", "done": true}` events; non-stream errors return structured ChatResponse with error field

### File List
**Modified Files:**
- `backend/pyproject.toml` - Added agent-related dependencies
- `backend/src/agent/__init__.py` - Added streaming exports
- `backend/src/agent/agent.py` - Complete rewrite with LangGraph, streaming, session memory
- `backend/src/agent/tools/__init__.py` - Updated exports for @tool decorated functions
- `backend/src/agent/tools/pricing_tools.py` - Converted to @tool decorator pattern
- `backend/src/agent/tools/data_tools.py` - Converted to @tool decorator pattern
- `backend/src/agent/tools/doc_tools.py` - Converted to @tool decorator pattern
- `backend/src/agent/prompts/__init__.py` - Updated exports for get_system_message
- `backend/src/agent/prompts/system.py` - Added get_system_message for LangGraph
- `backend/src/api/routers/chat.py` - Added SSE streaming with EventSourceResponse
- `backend/src/schemas/__init__.py` - Added ChatStreamEvent export
- `backend/src/schemas/chat.py` - Added ChatStreamEvent schema

**New Files:**
- `backend/src/agent/streaming.py` - SSE generator utilities

**Test Files Modified:**
- `backend/tests/unit/test_agent/test_tools.py` - Updated for @tool decorator pattern
- `backend/tests/unit/test_agent/test_schemas.py` - Added ChatStreamEvent tests
- `backend/tests/unit/test_agent/test_prompts.py` - Updated for get_system_message

**New Test Files:**
- `backend/tests/unit/test_agent/test_streaming.py` - SSE format and generator tests

## QA Results
(To be filled by QA Agent)
