# Story 4.9.1: LangChain Agent Backend with Streaming

## Status: Draft

## Assigned To: Mario

## Story

**As a** frontend developer,
**I want** a backend chat endpoint powered by LangChain with streaming support,
**so that** users see real-time token-by-token responses in the chat interface.

## Acceptance Criteria

1. LangChain agent implemented in `backend/src/agent/agent.py` with all required tools
2. **Tools registered:**
   - `optimize_price`: Get optimal price for context
   - `explain_decision`: Get full explanation
   - `sensitivity_analysis`: Get sensitivity data
   - `get_segment`: Get segment classification
   - `get_eda_summary`: Get dataset statistics
   - `get_external_context`: Get current external factors (stub/mock)
   - `get_evidence`: Get documentation
   - `get_honeywell_mapping`: Get mapping documentation
3. Agent correctly routes natural language queries to appropriate tools
4. Agent generates natural language responses incorporating tool results
5. Agent maintains conversation context (memory) within session
6. `POST /api/v1/chat` endpoint supports **Server-Sent Events (SSE)** streaming
7. Non-streaming fallback available via query param `?stream=false`
8. Error handling for edge cases (ambiguous queries, tool failures)

## Tasks / Subtasks

- [ ] Task 1: Create agent module structure (AC: 1)
  - [ ] Create `backend/src/agent/__init__.py`
  - [ ] Create `backend/src/agent/agent.py`
  - [ ] Create `backend/src/agent/tools/` directory
  - [ ] Create `backend/src/agent/prompts/` directory

- [ ] Task 2: Create pricing tools (AC: 2)
  - [ ] Create `backend/src/agent/tools/__init__.py`
  - [ ] Create `backend/src/agent/tools/pricing_tools.py`
  - [ ] Define `optimize_price` tool wrapping existing service
  - [ ] Define `explain_decision` tool
  - [ ] Define `sensitivity_analysis` tool

- [ ] Task 3: Create data tools (AC: 2)
  - [ ] Create `backend/src/agent/tools/data_tools.py`
  - [ ] Define `get_segment` tool wrapping segmenter
  - [ ] Define `get_eda_summary` tool wrapping EDA endpoint
  - [ ] Define `get_external_context` tool (stub returning mock data)

- [ ] Task 4: Create documentation tools (AC: 2)
  - [ ] Create `backend/src/agent/tools/doc_tools.py`
  - [ ] Define `get_evidence` tool
  - [ ] Define `get_honeywell_mapping` tool

- [ ] Task 5: Create system prompt (AC: 3, 4)
  - [ ] Create `backend/src/agent/prompts/__init__.py`
  - [ ] Create `backend/src/agent/prompts/system.py`
  - [ ] Define agent persona as "PrismIQ Pricing Copilot"
  - [ ] Include tool usage instructions
  - [ ] Add response formatting guidelines

- [ ] Task 6: Implement agent with LangGraph (AC: 3, 4, 5)
  - [ ] Initialize LangGraph ReAct agent with tools
  - [ ] Configure checkpointer for session memory (MemorySaver)
  - [ ] Set up tool routing via `create_react_agent`
  - [ ] Test tool selection for different query types

- [ ] Task 7: Implement streaming callback handler (AC: 6)
  - [ ] Create `backend/src/agent/streaming.py`
  - [ ] Implement `StreamingCallbackHandler` for token-by-token output
  - [ ] Create async generator for SSE events
  - [ ] Format as `data: {"token": "...", "done": false}\n\n`

- [ ] Task 8: Create chat router with SSE (AC: 6, 7)
  - [ ] Create `backend/src/api/routers/chat.py`
  - [ ] Implement `POST /api/v1/chat` with `StreamingResponse`
  - [ ] Support `?stream=false` for non-streaming fallback
  - [ ] Return `Content-Type: text/event-stream` for streaming

- [ ] Task 9: Create schemas (AC: 6, 7)
  - [ ] Create `backend/src/schemas/chat.py`
  - [ ] Define `ChatRequest` (message, context, session_id)
  - [ ] Define `ChatResponse` (message, tools_used, pricing_result, timestamp)
  - [ ] Define `ChatStreamEvent` (token, done, error)

- [ ] Task 10: Add error handling (AC: 8)
  - [ ] Handle ambiguous queries with clarification prompt
  - [ ] Handle tool failures gracefully with fallback message
  - [ ] Return structured error events in stream

- [ ] Task 11: Write tests
  - [ ] Test tool routing for different query patterns
  - [ ] Test memory persistence across calls
  - [ ] Test streaming output format
  - [ ] Test error handling

## Dev Notes

### Agent Architecture
```
backend/src/agent/
├── __init__.py
├── agent.py              # Main agent setup + PrismIQAgent class
├── streaming.py          # SSE streaming handler
├── tools/
│   ├── __init__.py
│   ├── pricing_tools.py  # optimize_price, explain_decision, sensitivity_analysis
│   ├── data_tools.py     # get_segment, get_eda_summary, get_external_context
│   └── doc_tools.py      # get_evidence, get_honeywell_mapping
└── prompts/
    ├── __init__.py
    └── system.py         # System prompt constant
```

### Dependencies to Add
```python
# requirements.txt additions - LangChain v1.0 (GA released October 2025)
langchain>=1.0.0
langchain-core>=1.0.0
langchain-openai>=1.0.0
langgraph>=1.0.0          # For agent orchestration (GA with LangChain 1.0)
sse-starlette>=1.8.2      # For SSE streaming in FastAPI
openai>=1.0.0
```

### Environment Variable
```bash
OPENAI_API_KEY=sk-...  # Required for LangChain OpenAI
```

### LangChain v1.0 Notes (GA - October 2025)
- **Stability commitment**: No breaking changes until v2.0
- Use `langchain-openai` package (not `langchain.chat_models`)
- Use `@tool` decorator for tool definitions (replaces `Tool()` class)
- Use `create_react_agent` from `langgraph.prebuilt` (replaces deprecated `create_openai_functions_agent`)
- Use `langgraph` for agent orchestration (recommended over legacy `AgentExecutor`)
- Memory is handled via `langgraph` checkpointing, not `ConversationBufferMemory`
- See migration guide: https://changelog.langchain.com/announcements/langchain-1-0-now-generally-available

### SSE Streaming Format
```
data: {"token": "The ", "done": false}

data: {"token": "optimal ", "done": false}

data: {"token": "price ", "done": false}

data: {"tool_call": "optimize_price", "done": false}

data: {"token": "is $24.50", "done": false}

data: {"message": "The optimal price is $24.50...", "tools_used": ["optimize_price"], "done": true}

```

### Tool Definitions (LangChain v1.0 - @tool decorator)
```python
# tools/pricing_tools.py
from langchain_core.tools import tool
from src.ml.price_optimizer import PriceOptimizer
from src.schemas.optimization import MarketContext

# Injected via dependency
_price_optimizer: PriceOptimizer | None = None

def set_price_optimizer(optimizer: PriceOptimizer):
    global _price_optimizer
    _price_optimizer = optimizer

@tool
def optimize_price(context_json: str) -> str:
    """Get the optimal price recommendation for a market context.
    
    Use this when the user asks about pricing, optimal price, or recommendations.
    Input: JSON string with market context fields.
    Returns: Recommended price with profit analysis.
    """
    import json
    context = MarketContext(**json.loads(context_json))
    result = _price_optimizer.optimize(context)
    return f"Optimal price: ${result.optimal_price:.2f} (expected profit: ${result.expected_profit:.2f}, uplift: {result.profit_uplift_percent:.1f}%)"

@tool
def explain_decision(context_json: str) -> str:
    """Get detailed explanation of a pricing recommendation.
    
    Use this when the user asks 'why', wants to understand factors, or needs justification.
    Input: JSON string with market context.
    Returns: Feature importance and decision rationale.
    """
    # Implementation wraps explanation service
    ...

@tool  
def sensitivity_analysis(context_json: str) -> str:
    """Analyze how price recommendation changes under different assumptions.
    
    Use this when user asks about robustness, sensitivity, or 'what if' scenarios.
    Input: JSON string with market context.
    Returns: Price sensitivity across elasticity scenarios.
    """
    # Implementation wraps sensitivity service
    ...
```

### Agent Setup (LangGraph v1.0 - create_react_agent)
```python
# agent.py
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from src.agent.tools.pricing_tools import optimize_price, explain_decision, sensitivity_analysis
from src.agent.tools.data_tools import get_segment, get_eda_summary, get_external_context
from src.agent.tools.doc_tools import get_evidence, get_honeywell_mapping
from src.agent.prompts.system import SYSTEM_PROMPT

class PrismIQAgent:
    def __init__(self):
        # LangChain v1.0: Use langchain-openai package
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True)
        
        # All tools using @tool decorator
        self.tools = [
            optimize_price,
            explain_decision,
            sensitivity_analysis,
            get_segment,
            get_eda_summary,
            get_external_context,
            get_evidence,
            get_honeywell_mapping,
        ]
        
        # LangGraph: MemorySaver for conversation persistence
        self.memory = MemorySaver()
        
        # LangGraph: create_react_agent (replaces AgentExecutor)
        self.agent = create_react_agent(
            model=self.llm,
            tools=self.tools,
            checkpointer=self.memory,
            state_modifier=SYSTEM_PROMPT,  # System prompt injection
        )
    
    async def chat(self, message: str, session_id: str) -> dict:
        """Non-streaming chat."""
        config = {"configurable": {"thread_id": session_id}}
        result = await self.agent.ainvoke(
            {"messages": [("user", message)]},
            config=config
        )
        return {
            "message": result["messages"][-1].content,
            "tools_used": self._extract_tools_used(result),
        }
    
    async def stream_chat(self, message: str, session_id: str):
        """Streaming chat with SSE events."""
        config = {"configurable": {"thread_id": session_id}}
        
        async for event in self.agent.astream_events(
            {"messages": [("user", message)]},
            config=config,
            version="v2",
        ):
            kind = event["event"]
            
            if kind == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                if token:
                    yield {"token": token, "done": False}
            
            elif kind == "on_tool_start":
                tool_name = event["name"]
                yield {"tool_call": tool_name, "done": False}
            
            elif kind == "on_chain_end" and event["name"] == "agent":
                final_msg = event["data"]["output"]["messages"][-1].content
                yield {"message": final_msg, "done": True}
```

### Streaming SSE Generator
```python
# streaming.py
from typing import AsyncGenerator
import json

async def sse_generator(agent: PrismIQAgent, message: str, session_id: str) -> AsyncGenerator[str, None]:
    """Convert agent stream to SSE format."""
    try:
        async for event in agent.stream_chat(message, session_id):
            yield f"data: {json.dumps(event)}\n\n"
    except Exception as e:
        yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"
```

### Chat Endpoint with SSE
```python
# api/routers/chat.py
from fastapi import APIRouter, Query
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse

router = APIRouter(prefix="/api/v1", tags=["chat"])

@router.post("/chat")
async def chat(
    request: ChatRequest,
    stream: bool = Query(True, description="Enable streaming response")
):
    if stream:
        return EventSourceResponse(
            agent.stream_chat(request.message, request.context),
            media_type="text/event-stream"
        )
    else:
        result = await agent.chat(request.message, request.context)
        return result
```

### Existing Services to Wire Up
- `PriceOptimizer` from `src/ml/price_optimizer.py` → `optimize_price` tool
- `Segmenter` from `src/ml/segmenter.py` → `get_segment` tool
- EDA summary from `data/eda_summary.json` → `get_eda_summary` tool

### Testing

- Test file: `backend/tests/unit/test_agent/`
- Test tool selection: "What's the optimal price?" → should call `optimize_price`
- Test memory: Follow-up question should have context
- Test streaming: Verify SSE format and token flow

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-02 | 1.0 | Initial story creation - Backend LangChain with streaming | SM Agent (Bob) |
| 2025-12-02 | 1.1 | Updated to LangChain v1.0 GA + LangGraph v1.0 (October 2025 release) | SM Agent (Bob) |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results
(To be filled by QA Agent)

