# Story 4.9: LangChain Agent Integration

## Status: Draft

## Assigned To: Mario

## Story

**As a** pricing analyst,
**I want** chat powered by an intelligent agent,
**so that** my questions are routed to the right backend tools.

## Acceptance Criteria

1. LangChain agent implemented in `backend/src/agent/agent.py`
2. **Tools registered:**
   - `optimize_price`: Get optimal price for context
   - `explain_decision`: Get full explanation
   - `sensitivity_analysis`: Get sensitivity data
   - `get_segment`: Get segment classification
   - `get_eda_summary`: Get dataset statistics
   - `get_external_context`: Get current external factors
   - `get_evidence`: Get documentation
   - `get_honeywell_mapping`: Get mapping documentation
3. Agent correctly routes natural language queries to appropriate tools
4. Agent generates natural language responses incorporating tool results
5. Agent maintains conversation context (memory) within session
6. `POST /api/v1/chat` endpoint invokes agent and returns structured response
7. Error handling for edge cases (ambiguous queries, tool failures)

## Tasks / Subtasks

- [ ] Task 1: Create agent module structure (AC: 1)
  - [ ] Create `backend/src/agent/__init__.py`
  - [ ] Create `backend/src/agent/agent.py`
  - [ ] Create `backend/src/agent/tools/` directory
  - [ ] Create `backend/src/agent/prompts/` directory

- [ ] Task 2: Create tool definitions (AC: 2)
  - [ ] Create `backend/src/agent/tools/pricing_tools.py`
  - [ ] Define `optimize_price` tool
  - [ ] Define `explain_decision` tool
  - [ ] Define `sensitivity_analysis` tool

- [ ] Task 3: Create data tools (AC: 2)
  - [ ] Create `backend/src/agent/tools/data_tools.py`
  - [ ] Define `get_segment` tool
  - [ ] Define `get_eda_summary` tool
  - [ ] Define `get_external_context` tool

- [ ] Task 4: Create documentation tools (AC: 2)
  - [ ] Create `backend/src/agent/tools/doc_tools.py`
  - [ ] Define `get_evidence` tool
  - [ ] Define `get_honeywell_mapping` tool

- [ ] Task 5: Create system prompt (AC: 3, 4)
  - [ ] Create `backend/src/agent/prompts/system.py`
  - [ ] Define agent persona
  - [ ] Include tool usage instructions
  - [ ] Add response formatting guidelines

- [ ] Task 6: Implement agent with memory (AC: 3, 5)
  - [ ] Initialize LangChain agent with tools
  - [ ] Add ConversationBufferMemory
  - [ ] Configure tool routing

- [ ] Task 7: Create chat endpoint (AC: 6)
  - [ ] Create `backend/src/api/routers/chat.py`
  - [ ] Implement `POST /api/v1/chat`
  - [ ] Accept message and context
  - [ ] Return structured response

- [ ] Task 8: Add error handling (AC: 7)
  - [ ] Handle ambiguous queries
  - [ ] Handle tool failures gracefully
  - [ ] Provide helpful error messages

- [ ] Task 9: Write tests
  - [ ] Test tool routing
  - [ ] Test memory persistence
  - [ ] Test error handling

## Dev Notes

### Agent Architecture
```
backend/src/agent/
├── __init__.py
├── agent.py              # Main agent setup
├── tools/
│   ├── __init__.py
│   ├── pricing_tools.py  # Pricing-related tools
│   ├── data_tools.py     # Data/segment tools
│   └── doc_tools.py      # Documentation tools
└── prompts/
    └── system.py         # System prompt
```

### Tool Definitions (LangChain)
```python
# tools/pricing_tools.py
from langchain.tools import Tool
from src.services.pricing_service import PricingService

def create_optimize_price_tool(pricing_service: PricingService) -> Tool:
    return Tool(
        name="optimize_price",
        description="""Get the optimal price recommendation for a market context.
        Use this when the user asks about pricing, optimal price, or recommendations.
        Input should be 'current_context' to use the active context.""",
        func=lambda _: pricing_service.get_recommendation(get_current_context())
    )

def create_explain_decision_tool(explanation_service) -> Tool:
    return Tool(
        name="explain_decision",
        description="""Get detailed explanation of a pricing recommendation.
        Use this when the user asks 'why', wants to understand factors, or needs justification.
        Input should be 'current_context' to explain the current recommendation.""",
        func=lambda _: explanation_service.explain(get_current_context())
    )

def create_sensitivity_tool(sensitivity_service) -> Tool:
    return Tool(
        name="sensitivity_analysis",
        description="""Analyze how price recommendation changes under different assumptions.
        Use this when user asks about robustness, sensitivity, or 'what if' scenarios.""",
        func=lambda _: sensitivity_service.analyze(get_current_context())
    )
```

### System Prompt
```python
# prompts/system.py
SYSTEM_PROMPT = """You are PrismIQ, an AI pricing copilot for dynamic pricing optimization.

Your role is to help pricing analysts understand and optimize prices using machine learning.

## Available Tools
- optimize_price: Get optimal price for current market context
- explain_decision: Get detailed explanation of why a price was recommended  
- sensitivity_analysis: See how recommendations change under different assumptions
- get_segment: Classify the current context into a market segment
- get_eda_summary: Get dataset statistics and overview
- get_external_context: Get current weather, events, fuel prices
- get_evidence: Get model cards and methodology documentation
- get_honeywell_mapping: Get ride-sharing to enterprise mapping

## Guidelines
1. Always be helpful and explain your reasoning
2. When discussing prices, always include the confidence level
3. When explaining recommendations, highlight the top contributing factors
4. If a query is ambiguous, ask for clarification
5. Format numbers appropriately (currency with $, percentages with %)
6. Keep responses concise but informative

## Response Format
- Start with the key insight or answer
- Provide supporting details
- End with a suggestion or next step when appropriate

Current market context is automatically available to tools."""
```

### Agent Setup
```python
# agent.py
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI

class PrismIQAgent:
    def __init__(
        self,
        pricing_service: PricingService,
        explanation_service: ExplanationService,
        sensitivity_service: SensitivityService,
        # ... other services
    ):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        self.tools = [
            create_optimize_price_tool(pricing_service),
            create_explain_decision_tool(explanation_service),
            create_sensitivity_tool(sensitivity_service),
            # ... other tools
        ]
        
        self.agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=create_prompt(SYSTEM_PROMPT)
        )
        
        self.executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            memory=self.memory,
            verbose=True
        )
    
    async def chat(self, message: str, context: MarketContext) -> ChatResponse:
        # Store context for tools to access
        set_current_context(context)
        
        result = await self.executor.ainvoke({"input": message})
        
        return ChatResponse(
            message=result["output"],
            tools_used=[],  # Extract from agent steps
            context=context,
            timestamp=datetime.now()
        )
```

### Chat Endpoint
```python
# api/routers/chat.py
from fastapi import APIRouter, Depends
from src.agent.agent import PrismIQAgent

router = APIRouter()

@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    agent: PrismIQAgent = Depends(get_agent)
) -> ChatResponse:
    return await agent.chat(request.message, request.context)
```

### ChatRequest/Response Schemas
```python
class ChatRequest(BaseModel):
    message: str
    context: MarketContext
    session_id: str | None = None  # For conversation continuity

class ChatResponse(BaseModel):
    message: str
    tools_used: list[str]
    pricing_result: PricingResult | None = None
    explanation: PriceExplanation | None = None
    timestamp: datetime
```

### Testing

- Test file: `backend/tests/unit/test_agent/test_agent.py`
- Test tool selection for different query types
- Test memory persists across calls
- Test error handling for invalid queries

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-02 | 1.0 | Initial story creation | SM Agent |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results
(To be filled by QA Agent)

